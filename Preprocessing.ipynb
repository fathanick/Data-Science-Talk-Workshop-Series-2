{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "Preprocessing.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fathanick/Data-Science-Talk-Workshop-Series-2/blob/master/Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnf8TyhzFgWj",
        "colab_type": "text"
      },
      "source": [
        "![picture](https://drive.google.com/uc?id=1-xuFvbmPE2Y8fTaRaBblI4ghFmRfG7Bs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuC639b_pn3x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "daf7fb06-790a-4504-86e5-c6d7b94250ad"
      },
      "source": [
        "!pip install Sastrawi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Sastrawi\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/4b/bab676953da3103003730b8fcdfadbdd20f333d4add10af949dd5c51e6ed/Sastrawi-1.0.1-py2.py3-none-any.whl (209kB)\n",
            "\r\u001b[K     |█▋                              | 10kB 17.7MB/s eta 0:00:01\r\u001b[K     |███▏                            | 20kB 3.2MB/s eta 0:00:01\r\u001b[K     |████▊                           | 30kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 40kB 3.0MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 51kB 3.7MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 61kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 71kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 81kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 92kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 102kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 112kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 122kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 133kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 143kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 153kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 163kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 174kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 184kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 194kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 204kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 215kB 4.9MB/s \n",
            "\u001b[?25hInstalling collected packages: Sastrawi\n",
            "Successfully installed Sastrawi-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CS1Ts4b7U7vi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk #import library nltk\n",
        "from nltk.tokenize import word_tokenize #import word_tokenize for tokenizing text into words \n",
        "from nltk.tokenize import sent_tokenize #import sent_tokenize for tokenizing paragraph into sentences\n",
        "from nltk.stem.porter import PorterStemmer #import Porter Stemmer Algorithm \n",
        "from nltk.stem import WordNetLemmatizer #import WordNet lemmatizer \n",
        "from nltk.corpus import stopwords #import stopwords\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory #import Indonesian Stemmer\n",
        "import re #import regular expression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQJ4qQrEr4Iu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "cf1ddcfd-d84c-4069-efc9-253b8642aae1"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUPa7Z9xU7vr",
        "colab_type": "code",
        "outputId": "a19afda8-d6f4-4db4-ae26-e118279a11e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "text_data = \"Saya suka belajar. Karena ingin menjadi pintar. Selain itu, saya ingin membuat bahagia kedua orang tua.\"\n",
        "sent_tokenize(text_data)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Saya suka belajar.',\n",
              " 'Karena ingin menjadi pintar.',\n",
              " 'Selain itu, saya ingin membuat bahagia kedua orang tua.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "4-dEXgjuU7v2",
        "colab_type": "code",
        "outputId": "ff30585a-128c-4a10-cb99-38b6be55d33c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#sentence tokenization\n",
        "def sentence_tokenization(s):\n",
        "    sentences_list = sent_tokenize(s)\n",
        "    \n",
        "    return sentences_list\n",
        "\n",
        "text_data = \"Saya suka belajar. Karena ingin menjadi pintar. Selain itu, saya ingin membuat bahagia kedua orang tua.\"\n",
        "sentence_tokenization(text_data)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Saya suka belajar.',\n",
              " 'Karena ingin menjadi pintar.',\n",
              " 'Selain itu, saya ingin membuat bahagia kedua orang tua.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Si25-m7bU7wA",
        "colab_type": "code",
        "outputId": "272570cd-29ec-456c-c439-bf062ea4c025",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "#word tokenization\n",
        "def word_tokenization(s):\n",
        "    tokens = word_tokenize(s)\n",
        "\n",
        "    return tokens\n",
        "    \n",
        "text_data = \"Saya suka belajar. Karena ingin menjadi pintar. Selain itu, saya ingin membuat bahagia kedua orang tua.\"\n",
        "word_tokenization(text_data)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Saya',\n",
              " 'suka',\n",
              " 'belajar',\n",
              " '.',\n",
              " 'Karena',\n",
              " 'ingin',\n",
              " 'menjadi',\n",
              " 'pintar',\n",
              " '.',\n",
              " 'Selain',\n",
              " 'itu',\n",
              " ',',\n",
              " 'saya',\n",
              " 'ingin',\n",
              " 'membuat',\n",
              " 'bahagia',\n",
              " 'kedua',\n",
              " 'orang',\n",
              " 'tua',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Q895LdkU7wL",
        "colab_type": "code",
        "outputId": "c89be516-07dc-45ae-8aa3-9796a07159f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#casefolding\n",
        "def casefolding(s):\n",
        "    new_str = s.lower()\n",
        "    \n",
        "    return new_str\n",
        "\n",
        "text_data = \"Saya suka belajar. Karena ingin menjadi pintar. Selain itu, saya ingin membuat bahagia kedua orang tua.\"\n",
        "casefolding(text_data)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'saya suka belajar. karena ingin menjadi pintar. selain itu, saya ingin membuat bahagia kedua orang tua.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9pJMoHLU7wS",
        "colab_type": "code",
        "outputId": "c4e39bf6-9293-4774-f0b3-759cd7d1469c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Stemming Indonesian\n",
        "def stemmingIndo(str):\n",
        "    factory = StemmerFactory()\n",
        "    stemmer = factory.create_stemmer()\n",
        "    return stemmer.stem(str)\n",
        "\n",
        "text_data = \"Saya suka belajar. Karena ingin menjadi pintar. Selain itu, saya ingin membuat bahagia kedua orang tua.\"\n",
        "stemmingIndo(text_data)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'saya suka ajar karena ingin jadi pintar selain itu saya ingin buat bahagia dua orang tua'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTnutBpfU7wW",
        "colab_type": "code",
        "outputId": "e8fa0861-c191-4582-ca40-5dff4f5971a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Stemming English\n",
        "def stemmingEnglish(str):\n",
        "    porter_stemmer = PorterStemmer()\n",
        "    words = word_tokenize(str)\n",
        "    result = list()\n",
        "    for word in words:\n",
        "        result.append(porter_stemmer.stem(word))\n",
        "        \n",
        "    return ' '.join(result)\n",
        "\n",
        "text_data = \"She had been with her father and sister when she was attacked and received first aid at the scene, an official said.\"\n",
        "stemmingEnglish(text_data)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'she had been with her father and sister when she wa attack and receiv first aid at the scene , an offici said .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "882p8UviU7wb",
        "colab_type": "code",
        "outputId": "5dafd892-413e-4dd9-8c82-e2ab74171666",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "word_data = \"It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\n",
        "# First Word tokenization\n",
        "nltk_tokens = nltk.word_tokenize(word_data)\n",
        "#Next find the roots of the word\n",
        "for w in nltk_tokens:\n",
        "       print(\"Actual: %s  Stem: %s\"  % (w,porter_stemmer.stem(w)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual: It  Stem: It\n",
            "Actual: originated  Stem: origin\n",
            "Actual: from  Stem: from\n",
            "Actual: the  Stem: the\n",
            "Actual: idea  Stem: idea\n",
            "Actual: that  Stem: that\n",
            "Actual: there  Stem: there\n",
            "Actual: are  Stem: are\n",
            "Actual: readers  Stem: reader\n",
            "Actual: who  Stem: who\n",
            "Actual: prefer  Stem: prefer\n",
            "Actual: learning  Stem: learn\n",
            "Actual: new  Stem: new\n",
            "Actual: skills  Stem: skill\n",
            "Actual: from  Stem: from\n",
            "Actual: the  Stem: the\n",
            "Actual: comforts  Stem: comfort\n",
            "Actual: of  Stem: of\n",
            "Actual: their  Stem: their\n",
            "Actual: drawing  Stem: draw\n",
            "Actual: rooms  Stem: room\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8XwATllU7wg",
        "colab_type": "code",
        "outputId": "ae06d04f-dcea-4a3d-f066-eaa1e4adf771",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "#Lemmatization\n",
        "nltk.download('wordnet')\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "word_data = \"It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\n",
        "nltk_tokens = nltk.word_tokenize(word_data)\n",
        "for w in nltk_tokens:\n",
        "       print (\"Actual: %s  Lemma: %s\"  % (w,wordnet_lemmatizer.lemmatize(w)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "Actual: It  Lemma: It\n",
            "Actual: originated  Lemma: originated\n",
            "Actual: from  Lemma: from\n",
            "Actual: the  Lemma: the\n",
            "Actual: idea  Lemma: idea\n",
            "Actual: that  Lemma: that\n",
            "Actual: there  Lemma: there\n",
            "Actual: are  Lemma: are\n",
            "Actual: readers  Lemma: reader\n",
            "Actual: who  Lemma: who\n",
            "Actual: prefer  Lemma: prefer\n",
            "Actual: learning  Lemma: learning\n",
            "Actual: new  Lemma: new\n",
            "Actual: skills  Lemma: skill\n",
            "Actual: from  Lemma: from\n",
            "Actual: the  Lemma: the\n",
            "Actual: comforts  Lemma: comfort\n",
            "Actual: of  Lemma: of\n",
            "Actual: their  Lemma: their\n",
            "Actual: drawing  Lemma: drawing\n",
            "Actual: rooms  Lemma: room\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0B-nOZyfU7wk",
        "colab_type": "code",
        "outputId": "80dabd03-f23e-4573-b950-b0bf75fe14dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#remove digit from string\n",
        "def removeDigit(str):\n",
        "    new_string =  re.sub(r\"[0-9]\", \" \", str)\n",
        "    return new_string\n",
        "\n",
        "text_data = \"saya lahir tanggal 1 Januari 2016\"\n",
        "removeDigit(text_data)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'saya lahir tanggal   Januari     '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK6_SfH1U7wq",
        "colab_type": "code",
        "outputId": "67d8a5d8-85ec-452c-9a26-d0fb1c905d7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "#pos tagging\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "def postag(str):\n",
        "    tok_sentence = nltk.word_tokenize(str)\n",
        "    tagged_sentence = nltk.pos_tag(tok_sentence)\n",
        "    return tagged_sentence\n",
        "\n",
        "text_data = \"It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\n",
        "postag(text_data)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('It', 'PRP'),\n",
              " ('originated', 'VBD'),\n",
              " ('from', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('idea', 'NN'),\n",
              " ('that', 'IN'),\n",
              " ('there', 'EX'),\n",
              " ('are', 'VBP'),\n",
              " ('readers', 'NNS'),\n",
              " ('who', 'WP'),\n",
              " ('prefer', 'VBP'),\n",
              " ('learning', 'VBG'),\n",
              " ('new', 'JJ'),\n",
              " ('skills', 'NNS'),\n",
              " ('from', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('comforts', 'NNS'),\n",
              " ('of', 'IN'),\n",
              " ('their', 'PRP$'),\n",
              " ('drawing', 'NN'),\n",
              " ('rooms', 'NNS')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nDSTtMvU7wu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re, string, unicodedata\n",
        "\n",
        "def cleaning(str):\n",
        "    #remove non-ascii\n",
        "    str = unicodedata.normalize('NFKD', str).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    #remove URLs\n",
        "    str = re.sub(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))', '', str)\n",
        "    #remove punctuations\n",
        "    str = re.sub(r'[^\\w]|_',' ',str)\n",
        "    #remove digit from string\n",
        "    str = re.sub(\"\\S*\\d\\S*\", \"\", str).strip()\n",
        "    #remove digit or numbers\n",
        "    str = re.sub(r\"\\b\\d+\\b\", \" \", str)\n",
        "    #to lowercase\n",
        "    str = str.lower()\n",
        "    #Remove additional white spaces\n",
        "    str = re.sub('[\\s]+', ' ', str)\n",
        "    \n",
        "    return str"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxmKzASCU7w3",
        "colab_type": "code",
        "outputId": "39f3353b-1de3-434b-950c-dcc2c7efb322",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "str = \"Copyright © 2008 John Wiley & Sons, Ltd. adalah ini https://www.analyticsvidhya.com/blog/2015/10/6-practices-enhance-performance-text-classification-model/\"\n",
        "print(cleaning(str))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "copyright john wiley sons ltd adalah ini\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05-YISTwU7w-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}